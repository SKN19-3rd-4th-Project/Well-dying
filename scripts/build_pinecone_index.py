import os
import json
from typing import List

from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# =========================
# 1. ê¸°ë³¸ ì„¤ì •
# =========================
INDEX_NAME = "digital-legacy-kb"  # ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥
EMBED_MODEL = "text-embedding-3-small"   # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ë¦„
EMBED_DIM = 1536                         # text-embedding-3-smallì˜ ì°¨ì› ìˆ˜
CHUNK_FILES = [
    "identity_verification_service_chunked.json",
    "naver_data1_chunked.json",
    "naver_data2_chunked.json",
    "online_shoppingmal_chunked.json",
    "google_data1_chunked.json",
    "google_data2_chunked.json",
    "kakaotalk_data1_chunked.json",
    "kakaotalk_data2_chunked.json"
]

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")

if OPENAI_API_KEY is None:
    raise RuntimeError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

if PINECONE_API_KEY is None:
    raise RuntimeError("PINECONE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI()  # í™˜ê²½ ë³€ìˆ˜ì˜ OPENAI_API_KEYë¥¼ ìë™ ì‚¬ìš©
pc = Pinecone(api_key=PINECONE_API_KEY)


# =========================
# 2. ì¸ë±ìŠ¤ ìƒì„± (ìµœì´ˆ 1íšŒ)
# =========================
existing_indexes = [idx["name"] for idx in pc.list_indexes()]
if INDEX_NAME not in existing_indexes:
    print(f"[INFO] ì¸ë±ìŠ¤ê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {INDEX_NAME}")
    pc.create_index(
        name=INDEX_NAME,
        dimension=EMBED_DIM,
        metric="cosine",  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),
    )
else:
    print(f"[INFO] ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¸ë±ìŠ¤ì…ë‹ˆë‹¤: {INDEX_NAME}")

index = pc.Index(INDEX_NAME)


# =========================
# 3. ì„ë² ë”© í•¨ìˆ˜ (ë°°ì¹˜ ë²„ì „)
# =========================
def embed_texts(texts: List[str]) -> List[List[float]]:
    """
    ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜.
    """
    resp = client.embeddings.create(
        model=EMBED_MODEL,
        input=texts,
    )
    # resp.data[i].embedding í˜•íƒœë¡œ ë²¡í„° ì ‘ê·¼
    return [d.embedding for d in resp.data]


# =========================
# 4. ì²­í¬ JSON ë¡œë“œ
# =========================
def load_chunks(path: str):
    with open(path, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    print(f"[INFO] ì´ {len(chunks)}ê°œì˜ ì²­í¬ ë¡œë“œ ì™„ë£Œ.")
    return chunks


# =========================
# 5. Pineconeë¡œ ì—…ë¡œë“œ (ì—…ì„œíŠ¸)
# =========================
def upsert_chunks_to_pinecone(chunks, batch_size: int = 100):
    total = len(chunks)
    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch = chunks[start:end]

        # 1) í…ìŠ¤íŠ¸ / ID ì¶”ì¶œ
        texts = [c["text"] for c in batch]   # JSONì˜ text í•„ë“œ
        ids = [c["id"] for c in batch]       # JSONì˜ id í•„ë“œ

        # 2) ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadatas = []
        for c in batch:
            base_meta = c.get("metadata", {}) or {}

            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° + ì›ë³¸ metadata í•©ì¹˜ê¸°
            meta = {
                "chunk_id": c["id"],
                "text": c["text"],
            }
            meta.update(base_meta)

            # ğŸ”¥ None ì œê±°
            clean_meta = {k: v for k, v in meta.items() if v is not None}

            metadatas.append(clean_meta)
        
        # 3) ì„ë² ë”© ìƒì„±
        embeddings = embed_texts(texts)

        # 4) Pinecone ë²¡í„° êµ¬ì„±
        vectors = []
        for cid, emb, meta in zip(ids, embeddings, metadatas):
            vectors.append({
                "id": cid,
                "values": emb,
                "metadata": meta,
            })

        # 5) ì—…ì„œíŠ¸(upsert)(ìš©ì–´: ìˆìœ¼ë©´ ê°±ì‹ , ì—†ìœ¼ë©´ ì‚½ì…)
        index.upsert(vectors=vectors)
        print(f"[INFO] ì—…ì„œíŠ¸ ì§„í–‰: {end}/{total} ê°œ ì™„ë£Œ.")

def upload_chunk_file(json_path: str, source_name: str | None = None):
    """
    json_pathì— ìˆëŠ” ì²­í‚¹ JSON íŒŒì¼ì„ ë¡œë“œí•´ì„œ
    Pinecone ì¸ë±ìŠ¤ë¡œ ì—…ë¡œë“œí•˜ëŠ” í¸ì˜ í•¨ìˆ˜.
    """
    print(f"[INFO] ì—…ë¡œë“œ ì‹œì‘: {json_path}")

    if not os.path.exists(json_path):
        print(f"[WARN] íŒŒì¼ ì—†ìŒ, ê±´ë„ˆëœ€: {json_path}")
        return

    chunks = load_chunks(json_path)

    # í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ source_nameì„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€í•˜ëŠ” ë¡œì§ì„
    # upsert_chunks_to_pinecone ìª½ìœ¼ë¡œ ë„˜ê²¨ë„ ë¨.
    upsert_chunks_to_pinecone(chunks, batch_size=100)

    print(f"[INFO] ì—…ë¡œë“œ ì™„ë£Œ: {json_path}")

if __name__ == "__main__":
    # ì²­í‚¹ JSONë“¤ì´ ë“¤ì–´ìˆëŠ” ê¸°ë³¸ í´ë”
    base_dir = os.path.join(
        os.path.dirname(__file__),
        "..",
        "data",
        "digital_asset_management_chunked_data",
    )

    for filename in CHUNK_FILES:
        json_path = os.path.join(base_dir, filename)
        upload_chunk_file(json_path)

    # ë£¨í”„ ëë‚œ ë’¤, ì¸ë±ìŠ¤ í†µê³„ í™•ì¸
    stats = index.describe_index_stats()
    print("[INFO] ì¸ë±ìŠ¤ í†µê³„:")
    print(stats)